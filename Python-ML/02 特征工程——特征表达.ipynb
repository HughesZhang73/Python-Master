{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n1. 缺失值处理\\n    如果特征值是连续的：1）选取该特征的所有的值的平均值来填补缺失值\\n                    2)使用中位数来填补缺失值\\n    如果特征是离散的:  选择该特征中出现最频繁的值来填充\\n\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. 缺失值处理\n",
    "    如果特征值是连续的：1）选取该特征的所有的值的平均值来填补缺失值\n",
    "                    2)使用中位数来填补缺失值\n",
    "    如果特征是离散的:  选择该特征中出现最频繁的值来填充\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n2. 特殊的特征处理\\n    对于时间原始特征：\\n        1） 使用原始的时间差值法：\\n            即计算出所有样本的时间到某一个未来时间之间的数值差距，\\n            这样这个差距是UTC的时间差，从而将时间特征转化为连续值。\\n        \\n        2） 根据时间所在的年，月，日，星期几，小时数，将一个时间特\\n            征转化为若干个离散特征（分析具有明显时间趋势的问题好用）\\n        \\n        3） 权重法，即根据时间的新旧得到一个权重值。比如对于商品，\\n        三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等\\n        的权重，在三个月内但是三天前的设置一个较大的权重。当然，还有\\n        其他的设置权重的方法，这个要根据要解决的问题来灵活确定。\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. 特殊的特征处理\n",
    "    对于时间原始特征：\n",
    "        1） 使用原始的时间差值法：即计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值。\n",
    "        \n",
    "        2） 根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征（分析具有明显时间趋势的问题好用）\n",
    "        \n",
    "        3） 权重法，即根据时间的新旧得到一个权重值。比如对于商品，三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等的权重，在三个月内但是三\n",
    "        天前的设置一个较大的权重。当然，还有其他的设置权重的方法，这个要根据要解决的问题来灵活确定。\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n3. 离散特征的连续化处理\\n    有的不能处理离散化的特征，比如线性回归，逻辑回归。\\n    1）使用独热编码one-hot encoding，比如某特征的取值是高，中和低，那么我们就可以创建三个取值为0或者1的特征，将高编码为1,0,0这样三个特征，中编码\\n    为0,1,0这样三个特征，低编码为0,0,1这样三个特征。也就是说，之前的一个特征被我们转化为了三个特征。sklearn的OneHotEncoder可以帮我们做这个处理。\\n    \\n    2）特征嵌入embedding，如对于用户的ID这个特征，如果要使用独热编码，则维度会爆炸，如果使用特征嵌入就维度低很多了。对于每个要嵌入的特征，我们会有一\\n    个特征嵌入矩阵，这个矩阵的行很大，对应我们该特征的数目。比如用户ID，如果有100万个，那么嵌入的特征矩阵的行就是100万。但是列一般比较小，比如可以取\\n    20。这样每个用户ID就转化为了一个20维的特征向量。进而参与深度学习模型。在tensorflow中，我们可以先随机初始化一个特征嵌入矩阵，对于每个用户，可以用\\n    tf.nn.embedding_lookup找到该用户的特征嵌入向量。特征嵌入矩阵会在反向传播的迭代中优化。\\n\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. 离散特征的连续化处理\n",
    "    有的不能处理离散化的特征，比如线性回归，逻辑回归。\n",
    "    1）使用独热编码one-hot encoding，比如某特征的取值是高，中和低，那么我们就可以创建三个取值为0或者1的特征，将高编码为1,0,0这样三个特征，中编码\n",
    "    为0,1,0这样三个特征，低编码为0,0,1这样三个特征。也就是说，之前的一个特征被我们转化为了三个特征。sklearn的OneHotEncoder可以帮我们做这个处理。\n",
    "    \n",
    "    2）特征嵌入embedding，如对于用户的ID这个特征，如果要使用独热编码，则维度会爆炸，如果使用特征嵌入就维度低很多了。对于每个要嵌入的特征，我们会有一\n",
    "    个特征嵌入矩阵，这个矩阵的行很大，对应我们该特征的数目。比如用户ID，如果有100万个，那么嵌入的特征矩阵的行就是100万。但是列一般比较小，比如可以取\n",
    "    20。这样每个用户ID就转化为了一个20维的特征向量。进而参与深度学习模型。在tensorflow中，我们可以先随机初始化一个特征嵌入矩阵，对于每个用户，可以用\n",
    "    tf.nn.embedding_lookup找到该用户的特征嵌入向量。特征嵌入矩阵会在反向传播的迭代中优化。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n4. 离散特征的离散化处理\\n    1. 独热编码、\\n    2. 虚拟编码dummy coding，它和独热编码类似，但是它的特点是，如果我们的特征有N个取值，它只需要N-1个新的0,1特征来代替，而独热编码会用N个新特征代\\n    替。比如一个特征的取值是高，中和低，那么我们只需要两位编码，比如只编码中和低，如果是1，0则是中，0,1则是低。0,0则是高了。目前虚拟编码使用的没有独热\\n    编码广，因此一般有需要的话还是使用独热编码比较好。\\n    \\n\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "\"\"\"\n",
    "4. 离散特征的离散化处理\n",
    "    1. 独热编码、\n",
    "    2. 虚拟编码dummy coding，它和独热编码类似，但是它的特点是，如果我们的特征有N个取值，它只需要N-1个新的0,1特征来代替，而独热编码会用N个新特征代\n",
    "    替。比如一个特征的取值是高，中和低，那么我们只需要两位编码，比如只编码中和低，如果是1，0则是中，0,1则是低。0,0则是高了。目前虚拟编码使用的没有独热\n",
    "    编码广，因此一般有需要的话还是使用独热编码比较好。\n",
    "    \n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. 连续特征的离散化处理\n",
    "    1. 根据阈值进行分组，比如我们根据连续值特征的分位数，将该特征分为高，中和低三个特征。将分位数从0-0.3的设置为高，0.3-0.7的设置为中，0.7-1的设置\n",
    "    为高。\n",
    "    2. 使用GBDT。在LR+GBDT的经典模型中，就是使用GDBT来先将连续值转化为离散值。那么如何转化呢？比如我们用训练集的所有连续值和标签输出来训练GBDT，\n",
    "    最后得到的GBDT模型有两颗决策树，第一颗决策树有三个叶子节点，第二颗决策树有4个叶子节点。如果某一个样本在第一颗决策树会落在第二个叶子节点，在第二颗\n",
    "    决策树落在第4颗叶子节点，那么它的编码就是0,1,0,0,0,0,1，一共七个离散特征，其中会有两个取值为1的位置，分别对应每颗决策树中样本落点的位置。\n",
    "    在sklearn中，我们可以用GradientBoostingClassifier的 apply方法很方便的得到样本离散化后的特征，然后使用独热编码即可。 \n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}